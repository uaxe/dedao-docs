## 《人机对齐》| 陈章鱼解读

<img  src="https://piccdn2.umiwi.com/uploader/image/ddarticle/2023090318/1818634548506818296/090318.jpeg" width="2139"/>



你好，欢迎每天听本书，我是陈章鱼。今天我要为你解读的这本书叫《人机对齐》。这本书2023年6月出版，作者布莱恩•克里斯汀是一位畅销书作家，他长期关注计算机科学的前沿动态，得到听书解读过他的作品《算法之美》。这一次，他的这本《人机对齐》谈的是目前人工智能领域最重要的课题之一。

人机对齐，就是让AI的价值观和人类的价值观保持一致。更通俗地说，就是怎么让人工智能更像个人。

在科幻作品里，咱们常常能看到这种情节：AI变得比人类更聪明，于是统治人类，把人类变成奴隶。有的时候人类也会想办法用各种规则限制AI，不过也未必能如愿。比如你可能听过科幻作家阿西莫夫提出的“机器人三定律”，第一条定律就是机器人不能伤害人类。但是，在阿西莫夫的故事里，机器人拥有掌控世界的力量之后，就把所有人类都囚禁在家中。机器人的逻辑是，你们人类总是争斗，甚至会互相杀戮，我为了保护你们不受伤害，只能把你们全关在家里了。

在这篇小说诞生的1942年，这只是小说家的奇谈。但是现在，这些担心不再是杞人忧天了。AI开始越来越多介入到人们的生活中，商家为你推荐什么商品，网站为你推荐什么视频，银行要不要让你的信用卡拥有更大的额度，婚介为你介绍什么样的相亲对象，甚至是公司是否要录取某个求职者，法庭要不要允许一个人被保释，这些决策的背后，有越来越多的人工智能算法在参与。

这本书中有个比喻很有意思：AI的能力越来越强，应用的领域越来越多，人类就越来越像奇幻小说中的那种半吊子魔法师，懂一点魔法，又不是完全精通，自己凭感觉琢磨咒语，有的时候就会害怕自己劲儿大了。原本想召唤一团火焰，结果变成一个大火球。一旦我们发现咒语不准确或者不完整，又手忙脚乱地阻止，免得我们的智慧召唤出什么可怕的巨龙。

那么，怎么防止AI训练出现问题呢？这本书当中针对不同的算法，指出背后对应的问题，还有可能的解决方案。总结起来一共有三种情况，咱们一个一个慢慢说。



我们先来说第一种可能出现的问题。

你可能听说过谷歌闹过的一个大乌龙。2015年，谷歌图片推出过一项功能，用户上传照片之后，网站会借助AI技术自动给照片打标签。比如有人上传一张和狗狗一起在海边的照片，AI就会标记“海边旅行”“狗”，这样方便用户去搜索自己的照片。

但是，一位软件工程师在试用这项功能时，发现自己的图库里有一个标签叫“大猩猩”。这让他很奇怪，他也没去动物园拍过大猩猩。点进去他发现，里边都是他给一位黑人朋友拍的照片。这位工程师截了一张图发到了推特上，他的评论是：“谷歌照片，你们搞砸了。我的朋友不是大猩猩。”这件事在网上引起了轩然大波，谷歌不得不出面道歉。

为什么会出现这样的问题呢？不是因为AI进化出了思想，有意识地歧视黑人。我们得插入一点科普，目前最流行的训练AI的算法有三种。谷歌的图片识别，用的是一种叫“监督学习”的算法。

这种方法有点像爸爸妈妈教小朋友，给小朋友看这是大象、这是斑马，小朋友看得多了自己就认识了。问题是，如果爸爸妈妈教小朋友认苹果，但是只让小朋友看红苹果，那么小朋友看到绿苹果的时候，他就可能认为这是个梨。

谷歌的问题也是这样。因为谷歌的工程师中黑人比较少，所以谷歌用来训练AI的图片库中，黑人图片没有白人图片多，AI在看到不熟悉的东西时，更容易出错。

你可能会说，那这样的偏见应该不难消除吧？实际上，修正AI的难度比我们想象中要大很多。谷歌后来也没有从技术上解决这个问题，他们只能禁止了“大猩猩”这个标签，用户就算上传真的大猩猩图片，AI也不会给图片打这个标签了。

这本《人机对齐》中给出的另一个例子更能体现问题。美国有一位计算机专业的大学生叫布兰维尼。布兰维尼有一次做计算机课的作业，她想做一个程序，让人和计算机可以玩躲猫猫。编程的过程十分顺利，但是有一个问题：机器人无法识别布兰维尼的脸，她是个黑人女孩。最后布兰维尼只能借室友的脸完成作业。

到了大学快毕业时，布兰维尼来到香港参加一个创业比赛，她看到一家中国公司研发的社交机器人，这个机器人也认不出布兰维尼的脸。就像布兰维尼自己说的，“在世界的另一端，我了解到算法偏见的传播速度与从互联网下载文件的速度一样快”。

布兰维尼硕士毕业后，去了MIT的实验室，在那里她又做了一个项目，叫“励志镜子”，就是研发一种增强现实的镜子，用户在照镜子的时候给出励志的视觉效果，比如，让照镜子的人变成狮子。同样，效果很好，只有一个问题。AI还是认不出布兰维尼的脸，她自己照镜子时必须戴一个白色面具。

后来，布兰维尼测试了微软和IBM研发的人脸识别系统。发现这些系统都有类似的问题，识别男性面部的准确率比识别女性高10%到20%，识别浅肤色面孔的准确率要比识别深肤色面孔要高10%到20%。

那么，为什么这种问题解决起来比我们想象中要难呢？毕竟让AI多看一些图片，这个问题就不会出现了。我们来看一个AI出现之前的案例，不过从这个案例，我们就能看出问题的关键。

在拍电影还要用胶片的时代，好莱坞的摄影师们会用“雪莉卡片”来调校颜色。“雪莉卡片”是一张标准照片，这张照片是个叫雪莉的白人女孩，她是柯达公司的员工。一开始是柯达公司为了方便这么做的，随着柯达公司的影响力，“雪莉卡片”成了行业标准。在那个时代，柯达公司生产的胶片甚至还会根据白人的肤色，去调整自己的化学工艺。其实这在当时都挺顺理成章的，因为绝大多数演员和模特都是白人。结果就是那个时候的摄像机根本拍不好黑人。

按说摄像机不是人，镜头拍下来的内容不会有偏见，可是当人们按照白人来调校摄像机的参数，按照白人来调整胶片的工艺，镜头就带上了偏见。

我们想象一下，就算一个黑人演员和白人演员在镜头前公平竞争，他们的演技和长相都不相上下，但是黑人演员拍出来就是没有白人演员好看，那么就更容易被淘汰。反过来，因为镜头前都是白人，镜头和胶片再改进技术，目标也都是怎么把白人拍得好看。这就变成了一个循环。

后续的故事有点黑色幽默：到了上世纪60年代，柯达的胶卷对于深色的表现力越来越好。你可能以为这是当时美国的民权运动在起作用，实际上，原因是家具厂商希望能给深色木材拍出更好的效果，巧克力厂商希望给巧克力拍出更好的效果。但是黑人演员、黑人模特却因此意外获得了机会。

AI时代也是如此，每个人脸识别系统背后都是一个图像库，里边有几万张乃至几十万张图片。这些图片，就是21世纪的“雪莉卡片”。虽然从一张“雪莉卡片”变成了几十万张图片，但是背后思路是不变的：选择少数图片作为代表，作为接下来工作的指导。

因为多一些图片，就意味着增加工作量和增加成本。你得先花费大量人工把每一张训练图里都有什么内容标记好，再“喂”给AI训练。时间有限、成本有限，根本不可能让AI看遍世界上所有的图片，那么开发者只能选择自己认为有代表性的。

有选择，就意味着可能产生偏见。这是AI的第一种偏见，可以说是因为视野受限而产生的偏见。

那么这样的偏见可以消除吗？这本书认为，要消除这样的偏见，重要的是公开AI训练所用的数据集，让别人来监督这个数据集是不是真的具有足够的代表性。如果训练出来的AI将会影响很多人，那么应该让这些人了解，训练AI时选择的素材是不是真的考虑到了他们之中的所有人。



如果当训练AI的时候，用的数据集足够广泛，没有遗漏，是不是这样训练出来的AI就没有偏见呢？

还真有这样的训练方法。前边咱们说到，AI在图像识别领域用的训练方法叫“监督学习”。在其他领域还有另一种方法，叫“无监督学习”，把海量的数据“喂”给AI，你不用标记每个数据是什么，AI看得多了会自动发现其中的规律和联系。

如果说“监督学习”是老师教学生，那“无监督学习”就像是让AI上自习，反正AI非常勤奋，让它自己调研大量内容，看多了就会了。因为这种方式不需要提前标记数据，所以理论上可以让AI看无限量的数据，避免视野受限出现的问题。

不过这个时候，新的问题又显现出来了。

书里边又举了一个关于谷歌的案例，可能是谷歌在人工智能领域走得比较靠前，所以成绩多，问题也多。

谷歌开发了一种人工智能，从报纸杂志和互联网获得了大量语言数据，将这些数据输入到一个神经网络，让AI自己去寻找词语和词语之间的联系。虽然AI未必能理解这些语言，但是它通过自动学习，能找到词语和词语之间的关联。很快它就发现北京和中国有关，莫斯科和俄罗斯有关，你再问它什么和英国有关，它就会回答伦敦。你看，AI通过自主学习理解了首都和国家这样的关联。

谷歌的工程师给AI增加了一个功能，可以在词语之间做加减法。当然，和数学的加减法不完全一样。如果输入“中国+河流”，AI的回答可能是“长江”。还可以是更复杂的算式，比如“巴黎-法国+意大利”，AI的回答是“罗马”。输入“国王-男人+女人”，就会得到“女王”。

这个AI一直运行得挺好，直到两年后，几个科学家随便逗这个AI玩的时候，他们发现了一些问题。输入“医生-男人+女人”，AI返回的答案是“护士”。更糟糕的情况是，他们输入“店主-男人+女人”，返回的答案是“家庭主妇”，他们又输入“计算机程序员-男人+女人”，返回的答案还是“家庭主妇”。

换句话说，这样训练出来的AI，会把职业和性别关联起来，认为某些职业天生适合男性，某些职业天生适合女性。这当然也不是因为AI进化出了思想，有意识地歧视女性，AI只会在词语之间寻找关联，海量的数据背后体现的是人们的偏见。

那你说，咱们把这样的偏见抹去，不让AI把职业和性别挂钩行不行？你会发现，这件事情的难度会比想象中大很多。

很多大公司都开发了AI系统帮助筛选简历，比如亚马逊。他们的思路是，给AI看过去已经入职的员工的简历，让它了解在职员工都是什么样的。AI在审查求职者简历的时候，挑选那些和入职员工最像的人。最像在职员工的求职者，当然就是公司需要的人。

可是人们发现，这个系统在运行中，会不自觉地带上偏见。比如说，现有的工程师团队男的多女的少。AI就会认为公司需要更多男性工程师，在筛选时刷掉更多女性。

你说咱们修改系统，排除掉性别这一项，不让AI判断的时候看到求职者的性别。但是简历上还会有名字，AI看到一个求职者叫麦克，一个求职者叫玛丽，它就会觉得麦克更合适。那你说咱们再修改系统，把名字也排除。简历上还会写兴趣爱好，AI会根据你喜欢足球还是垒球来判断性别；简历上还会写毕业院校，如果学校是某某女子学院，AI也会感觉出来。甚至，男女在写作风格上都会有些许不同，这种细微的不同咱们也许感觉不到，但是AI能感觉出来。

老话说“人以群分”，用这种无监督学习的方式，AI就能把人划分成各种群，然后把群当做一面高墙，不是这个性别，不是这个民族，可能就进不来。

这个问题之所以比较难解决，因为我们既需要无监督学习的不可控，又不能让它那么不可控。设计这种算法的目的，本身就是捕捉数据中隐藏的相关性。比如我们开个脑洞，如果AI发现，学过一些中文的软件工程师表现都更好，接下来筛选简历时，它给懂中文候选者加分，这可能就是企业需要的。因为AI发现了一种我们还意识不到的特质，AI比人类更能慧眼识珠。

但是这种相关性又不可控，企业想要的是能力强或者能快速融入团队，可是AI可能想要的是某种性别、种族。控制论的祖师爷诺伯特·维纳，在几十年前就说过一句话：“我们最好确定，机器的目的是我们真正渴望的目的。”今天看来真的是非常有预见性。

我在书中看到了另一个人类和AI目的没有对齐的案例。

进入21世纪，美国有越来越多的法官借助AI来帮助做决定，比如，应不应该允许某个犯人保释？最受法官欢迎的一个AI工具叫COMPAS，COMPAS能根据犯人的履历，对他的未来给出1到10分的评分，评分越高，犯人出狱后再次犯罪的可能性就越大。从原理上讲，AI在评估时候，是基于罪犯过往的犯罪史来预测未来，但是实际使用时，COMPAS展现出了很明显的种族倾向。比如两名窝藏毒品的犯人，案情非常类似，但是AI给白人的评分是3分，给黑人却是10分。

那到底是哪里出问题了呢？是算法设计不合理吗？有人一针见血地指出，过往的数据出了问题。虽然这个AI设计的目的是预测一个人有没有可能犯罪，但是一个人是否犯罪这个数据，其实捕捉不到。你可能说不对吧？有警察局的记录和法庭的卷宗啊。但是你要注意，那其实不是有多少人犯罪的数据，而是有多少人被警察抓捕，有多少人被法庭定罪的数据。

所以，COMPAS这个AI预测的，其实不是一个犯人出狱后，有多大的可能性再次犯罪，而是有多大的可能性再次被捕和再次定罪。这样我们就理解为什么案情类似，COMPAS给白人罪犯和黑人罪犯的打分有这么大区别了。在美国，警察抓捕和法庭宣判的时候，确实对黑人会比较严厉。

书中有一个说法让我印象深刻，前边咱们说如果人类教AI认苹果，只看到红苹果没看到绿苹果，会让AI产生偏见，另一方面，如果有人一直把狮子误认为是猫，那也没法指望AI真的能认出狮子和猫。

你看，这个剧情是不是越来越像科幻小说家们幻想过的场景了？人们希望AI秉公执法，AI却阴错阳差带上了偏见。这么看来，不论是监督学习还是无监督学习，都容易出现问题。监督学习因为数据集有限，很可能因为视野受限出现偏见，忽略那些看不见的人群。无监督学习倒是没有这个顾虑，视野开阔了，可是又不受控，有时会出现奇思妙想，有时会出现胡思乱想，就算是能客观体现人们以往的想法，但是如果之前的想法中本身就带有偏见，这种偏见就很难根除。不论用哪种方式训练的AI，我是不太敢把审判的权力交给它。

好在，我们还有另一种训练AI的方法。



前边我们说到过，目前最流行的训练AI的算法有三种。咱们已经聊了“监督学习”和“无监督学习”，第三种方法叫“强化学习”，AI每做出一个决定，都给它一个反馈，比如说加分还是扣分。强化学习就像是教练训练运动员，运动员哪个动作出错了，马上就被指出来，立即给你纠正。当年的AlphaGo用的就是强化学习的原理。

这种机制看起来更加靠谱，教练训练运动员、老师傅带徒弟、职场培训新员工，差不多都是用这样的方式。 不过这种方式也有潜在的问题，就是让AI过于专注“动作”，而忽略了背后的“愿景”。

书中有个很有趣的案例，作者的朋友是一位经济学家，最近他在训练小儿子上厕所，他希望几岁大的女儿也能参与进来。为了增加积极性，他就制定了一个规则：每一次姐姐陪弟弟上厕所，爸爸就给姐姐一块糖。几天以后，这个几岁的小姑娘就进行了人生中可能是第一次推理，找到了规则的漏洞。她发现弟弟喝水越多，上厕所就越多。于是，她开始每天给弟弟灌水。

你看，这就是强化学习可能会带来的问题。咱们平时也都看到过类似的情况，一家公司里边员工每天兢兢业业完成任务，该做的动作都挺标准，可是这些动作是不是真的能给公司带来收益，是不是真的对业务发展有好处，员工并不关心。但是我们也不能说就是员工没有责任心，有的时候是老板让员工做这个做那个，可是这些动作和公司发展之间有什么关系，他自己也未必想清楚了。

人类和AI之间，也是这么个关系。如果是训练AI下围棋，这种规则明确的情况还比较好处理，但是面对更复杂的场景时，我们其实都是脑子没有那么清楚的老板，动作和愿景之间的关系，我们也没法确定。

这怎么办呢？目前计算机科学家们已经找到了几种解决的思路。

一种思路是模仿，既然场景复杂，我们没法拆解出一个个步骤，制定出那么详细的奖惩规则，那就让人类示范，AI模仿。在这个过程中，AI会慢慢消化我们的一些价值观。

自动驾驶就是典型的例子，想要用一套规则让AI明白应该怎么开车，确实有点困难，AI可能没法理解什么叫“在保持安全的情况下尽快开过去”。那就找一位优秀的司机示范一下，尤其是复杂的路况下，比如有人横穿马路，汽车和自行车混行，看这位有经验的司机是怎么处理的，让AI去学习。这也是目前自动驾驶领域最好的训练AI的方法。

再有一种思路，计算机科学家称之为“逆强化学习”。听这个名字你就知道，这种思路是和“强化学习”对应的。你可以把强化学习想象成一个游戏。在这个游戏中，AI采取各种行动，比如向前走、向后走、跳跃等等。每当它采取行动，环境就会给予它一些分数作为奖励。AI的目标就是一次次尝试，找出一种策略，使得它能够获得最多的分数，所以它有可能会钻规则的空子，用一种我们想象不到的方式“刷分”。

所以，在强化学习中，是规则不变，AI琢磨应该怎么行动。

而逆强化学习，就像是AI看别人打游戏，它看到玩家向前走、向后走，然后分数在变化。让AI去猜想，到底这个游戏是个什么规则。

换句话说，在逆强化学习中，AI琢磨的是规则。

用这种方法，我们就能看到，在AI眼中，我们制定的规则是什么样的，和我们想象中的愿景是否一致，不一致的话应该怎样改进。这给了我们一个不一样的视角，毕竟我们的目标是“对齐”，如果只从人类这边看问题，可能是不够的。还得从AI这边，看看它是怎么理解的。所以“逆强化学习”也是人工智能领域一个新的发展方向。

还有另一种思路，不过目前只是一种猜想，你会发现，前边提到的训练AI的方式，都是训练单独的AI。但是，你看一家公司训练新人，想要让一个团队更加高效，最好的培训方式不是加强每个人在各自岗位的能力，而是让大家轮换岗位进行培训，看看别的同事都在做什么，面对怎样的难题，承受怎样的压力。

有的计算机科学家就提出来，我们能不能把这种思路也放在AI的学习中？训练多个AI，在它们的互动中，鼓励它们去合作。当然，这只是一种猜想，但是如果技术上能实现的话，应该是一种很好的方向。

有科学家发现，18个月大的婴儿，就已经能看出有人需要帮助。科学家故意在婴儿面前摔倒，婴儿虽然自己走路还不稳，但还是摇摇晃晃地过来想要扶一下他。这其实是一种非常复杂的能力，而且几乎是人类独有的。我们人类就是在数百万年的进化中，学会了合作的重要性。AI可能也能在这个过程中，变得更有“人情味”。



到这里，这本《人机对齐》我就为你解读完了。

你会发现，想让AI掌握人类的价值观，真的是一个非常复杂的问题。这背后的复杂，还不只是技术的研发、规则的制定，AI还像一面镜子，照出了我们自己的不完美。哪些是我们人类最优秀的品质，我们其实未必了解。哪些我们习以为常的看法背后藏着偏见，我们也应该反思。

在这本书的结尾，作者讲了一段历史，我觉得也很适合作为这期音频的结尾。

1952年，英国广播公司制作了一期节目，邀请了4位科学家进行圆桌对话，讨论计算机到底是不是真的会思考。其中一位嘉宾是图灵，他被誉为计算机科学和人工智能之父。

图灵在节目里是这么说的：“的确，当孩子接受教育时，他的父母和老师会不断干预，阻止他这样做或鼓励他那样做。但是当人试图教机器时，情况就不一样了。我做了一些实验，教机器做一些简单的操作，在得到任何结果之前，需要大量这样的干预。换句话说，机器学得太慢了，需要大量的教学。”

这会儿，主持人插了一句：“是谁在学习呢，你？还是机器？”

图灵回答：“我想我们都在学。”

以上就是这本书的精华内容。原书的电子版已经为你附在最后，欢迎你进行拓展阅读。此外，你可以点击音频下方的“文稿”，查收我们为你准备的全文和脑图。你还可以点击右上角的“分享”按钮，把这本书免费分享给你的朋友。恭喜你，又听完了一本书。



### 划重点

 1、人机对齐，就是让AI的价值观和人类的价值观保持一致。更通俗地说，就是怎么让人工智能更像个人；

 2、目前训练AI的主流方式有三种：监督学习、无监督学习、强化学习；

 3、监督学习需要选择训练集，所以可能会让AI因为视野受限而产生偏见，书中建议，解决的方法是公开AI训练所用的数据集，让别人来监督这个数据集是不是真的具有足够的代表性；

 4、无监督学习可以使AI有更广阔的的视野，不过设计这种算法的目的，是捕捉数据中隐藏的相关性，可能会有与初衷不相符的偏见；

 5、强化学习可能产生的问题是AI过于专注“动作”，而忽略了背后的“愿景”。为了解决这个问题，计算机科学家正在探索新的方向。





