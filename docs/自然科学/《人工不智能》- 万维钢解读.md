## 《人工不智能》| 万维钢解读

### 关于作者

梅瑞狄斯·布鲁萨德，毕业于哈佛大学计算机系，拥有计算机和数学的学位。她自己创造过好几个人工智能系统，在 MIT 媒体实验室工作过。布鲁萨德现在是纽约大学助理教授，同时她还是一个记者。

### 关于本书

你已经听过太多有关“人工智能”的话题了，什么“人工智能将会改变世界”“人工智能将让大多数人失业”……有些好消息说得太多，就容易让人产生不切实际的希望。不切实际的希望太强，就容易变成迷思。事实上，担心的那种未来离我们还非常遥远，现在所谓人工智能还没有那么厉害，人类已经反应过来了，可以预见的近期内不会有什么人工智能导致的大失业。我们这次要说的这本书，则是想让你进一步冷静一下。

### 核心内容

第一，高估人工智能会产生什么样的社会问题？

第二，自动驾驶这件事到底有多难？

第三，现阶段人工智能的社会化应用的根本矛盾是什么？

<img  src="https://piccdn3.umiwi.com/img/201902/13/201902131411067155836013.jpg" width="1080"/>

###  



你好，欢迎每天听本书。本期要解读的是一本2018年4月出版的英文新书《人工不智能：计算机如何误解世界》，作者是人工智能专家、数据记者和纽约大学助理教授梅瑞狄斯·布鲁萨德。如果用一句话来总结书中精髓，那就是：人工智能想大规模取代人类并不是一件容易的事情，它还有很长的一段路要走，我们对于人工智能的恐慌和其他，其实都是在过度想象。

生活在今天，相信你已经听过太多有关“人工智能”的话题，什么“人工智能将会改变世界”“人工智能将让大多数人失业”等等。但事实上，像《未来简史》那本书里担心的那种，数据和算法掌控一切的未来，离我们还非常遥远。现在的“人工智能”根本没有那么厉害，可以预见的近期内不会有什么人工智能导致的大失业。也就是说，其实我们都高估了人工智能。

电视剧《西部世界》里面那样的机器人，跟现在高科技公司正在研发的那种人工智能，完全是两码事。我们很多关于未来人工智能的种种设想，比如彻底取代人类，甚至超越人类，都只是物理学家纯理论的设想，或者干脆是科幻作家的噱头。现在真实的人工智能其实应该叫“人工不那么智能”。Alpha Go 下赢了围棋这种事情，其实没什么大不了的，它就是一个算法机器而已。

我们对于人工智能的误解，如果仅仅是观念和想象也就罢了，严重的是，因为人们过分依赖计算机算法，反而带来了一系列社会问题，所以我们需要正确认识人工智能。这也是作者梅瑞狄斯·布鲁萨德写这本书的用意。

布鲁萨德毕业于哈佛大学计算机系，拥有计算机和数学的学位。她自己创造过好几个人工智能系统，在 MIT 媒体实验室工作过。布鲁萨德现在是纽约大学助理教授，同时她还是一个记者。而且她是一种非常新型的记者，叫“数据记者”。数据记者的工作不是整天采访什么“大数据科学家”，而是自己直接从数据中挖掘故事。布鲁萨德做的事情，是自己编写一个人工智能专家系统，让这个系统替她从各种数据库中发现规律，她从中获得洞见，写成报道。

也就是说，布鲁萨德是一个“用数据发现真相”的人。布鲁萨德这个数据记者，最关心的是让那些已经在取代人类做决定的“算法”负责任。她发现人们过于相信算法。算法都是人写的，人会犯错，算法就会犯错。现在的人非常爱说什么“计算机改变了世界”，特别是乔布斯，动不动就“这个产品再一次改变了世界”。而布鲁萨德将会告诉你，其实计算机没有改变什么。社会还是这个社会，计算机并没有解决我们的社会问题。

好，接下来我就通过三个部分，来讲述本书内容。第一部分，我会让你了解，高估人工智能会产生什么样的社会问题？第二部分，我会让你明白，自动驾驶这件事到底有多难？第三部分，我会让你看清，现阶段人工智能的社会化应用的根本矛盾是什么？

###  



我们先来说第一部分，高估人工智能会产生什么样的社会问题？

现在有一种情绪被布鲁萨德称为“技术沙文主义”，认为一切社会问题都可以用技术解决，特别是可以用计算机技术解决。布鲁萨德这本书就是专门跟技术沙文主义唱反调。布鲁萨德认为，包括人工智能在内的所谓新技术，其实并没有解决真正的社会问题。

咱们先说一个“简单”例子。假设你任职于某个贫困地区的教育局，你负责给本地所有中小学发放课本，那么你如何确保学生们都能得到课本呢？你肯定会认为这是一个简单的问题，中国最穷的地方也没发生过学生上课没有教材的事儿。但是美国有好几个州，包括纽约、宾夕法尼亚和华盛顿这样比较富的州，却有很多很多学生没有教材可用。这是一个严重的社会问题，可是却不能用技术方法解决。

几年前布鲁萨德的儿子上小学，一家人住在宾夕法尼亚州。儿子要准备全州统一的标准化考试，布鲁萨德就研究了一番考试的内容之后发现，这个考试其实可以破解的答案其实都在课本里，你只要把课本的内容参透了，通过考试就非常简单。反过来说如果你不看课本，哪怕知识面再广，也很难在考试上有好的表现。

这就是为什么美国很多老师痛恨标准化考试。但这不是我们今天想说的重点，重点在于，这么简单的考试，美国能熟练解题的学生还不到一半，宾夕法尼亚州高中的毕业率只有64%。而根本原因，就在于很多学生“拿不到课本”。 

你肯定觉得这不荒唐吗？我要是中学生我第一件事就得给自己弄一套课本啊。可是对不起，真没有。咱们中国的教材是由国家统一出版，价格便宜。但美国的教材由私人公司出版，价格很贵。比如一本中学生用文学教材就要114.75美元。

政府的教育经费往往有限，有时候分到每个学生头上的教材费可能只有30多美元，这样一来就存在很大的缺口。

那你说政府钱不够我自己买一套行不行，对不起没有这个渠道。布鲁萨德她儿子的教材学校不让带回家，她想给儿子另外弄一套，各处都找不到，连盗版都没有。现在美国的中小学课本都是由三家大公司出版的，直接供货到学校，同时考试题也是这几家公司出。换句话说你是用这些公司的课本，然后用这些公司的考试题，一条龙服务，这是个每年几十亿美元的生意。

那你说能不能让学生重复使用旧教材呢？有时候的确就是这么干的，有的学生还在用八十年代师兄师姐的教材。但这个做法通常不可行，因为教材几乎每年都要随着州政府的教育政策变来变去。

这还不算，有些看似能用技术解决的问题，也解决不了。比如有时候经费到位了，书也买了，结果书被放置在学校的仓库里，没发到学生手上。老师以为今年还是没有书，校长不记得书有没有买。布鲁萨德找到一个类似于“区教委”的机构，说你们知不知道辖区内这些学校今年都开设了什么课程，需要什么教材。区教委说这个真没有，但是我们有个中央数据库，有各个学校订购教材的数据。

布鲁萨德就把这个数据库拿过来，结果一看，很多学校订购的教材数和学生人数根本对不上。布鲁萨德又跑到各个学校去现场调查，发现教材的发放和管理都非常混乱。学校经费有限，人手不够，有些校长要亲自管理教材。有的校长不愿意使用州里的数据库系统，自己弄了个表格，很难跟别人分享，最后就是一大堆问题。

布鲁萨德就说，数据技术再先进，最初的数据录入，也是由人来完成的。如果校长根本不愿意录入数据，老师根本统计不清楚自己班需要多少教材，再先进的计算机技术又有什么用呢？这里的主要问题在于，教育，本质上是个因人而异的、混乱的、动态的系统，而公共教育系统，是个统一的、标准化的、最好是不变的系统。

比尔·盖茨和梅琳达基金会一直在美国全国推行一个所谓“共同核心课程”系统，相当于中国那种全国统一的教学大纲和标准化考试，但是在各地受到了老师和学校的抵抗。其实老师的抵抗有道理，这不仅仅是不民主，而是不同学区的学生水平差异很大，用同一个标准确实不合理。学校教学没有自由度，而且应试教育的负面影响太大，结果现在美国的公共教育非常混乱。

布鲁萨德说，盖茨这帮人是想把教育当成一个工程问题去解决。工程问题本质上是数学，需要在一个定义良好的环境里，用定义良好的参数描写一个定义良好的问题。可是教育问题从来都不是“定义良好”的，其中有各种复杂的情况。复杂和标准化是一对永恒的矛盾，这就是为什么计算机技术发展了这么多年，美国基础教育几乎没有发生任何进步。

所以，我们可以看到，就在分发教材这一件事儿上，计算机人工智能都不能起到很好的作用。因为计算机和人工智能擅长的是解决工程问题，工程问题要求定义良好，而复杂社会问题恰恰是不能定义良好的。那么好，你说我们不解决社会性问题，就应对以工程问题为主的事情行不行？可能还是不行，接下来我们就来说一说，在自动驾驶，这个看上去纯工程的问题上，人工智能有多难。这是我们的第二部分内容。

###  



我们先来说一个结论，现阶段所有的人工智能，都有一个根本的弱点。这个弱点就是它们高度依赖数据，都是对过去经验的总结，它们没有办法预测“没见过”的事情。所以这种人工智能的应用非常有限。它最适合常见的、简单的、不变的应用场景，不能直接推广。一旦遭遇必须推广的场景，就面临各种问题，比如“自动驾驶汽车”。

公众对自动驾驶汽车这个话题已经欢呼了很多年，那现在自动驾驶技术是什么情况呢？国际汽车工程师学会弄了一个标准，把自动驾驶汽车一共分为五个级别。

零级代表完全没有自动化，就是人开车。一级，是指计算机在某些时候、某种程度上可以给人提供一些辅助性的帮助。像自动刹车、保持车道、停靠辅助系等等。二级，是有的时候汽车可以自己开，但是要求人一直盯着，特斯拉已经做到了这个级别。三级，是说人可以不盯着了，就让车自己开，但是如果车向你发出信号，你要随时接管驾驶。四级，是指在某些环境和条件下，实现自动驾驶，人去睡觉都没问题。五级，是完全的自动驾驶，不论什么天气和路况人都不用管车。

截至此时此刻，任何一家公司的自动驾驶技术都没有超过二级。而有些专家认为，五级自动驾驶是一个永远都达不到的目标。这是为什么呢？因为人工智能处理不了意外。 

其实你开车的时候并不是简单地把着方向盘控制着油门和刹车，你非常有智能。你要看交通信号，你要看各种路边的标志物，你要判断路上有什么东西。如果前面路上有一只小鸭子在慢慢走，你得踩刹车；但是如果是一只鸟，你可以想象车开过去它就会飞走，你就不用减速；如果路上有个塑料袋，你可以直接碾压过去；但如果那是个石头，你就必须绕着走。

你对路面状况有深刻的理解，这种理解和你的生活阅历，和你平时积累的经验有关。最起码你得知道塑料袋是什么，石头是什么，而汽车并不知道。计算机只关心这些物体的移动趋势，估算每个物体的速度，预测它的路线，看看跟车的路线会不会发生冲突。如果有冲突就踩刹车或者绕着走，但真实的路面上会有各种意外。

我们都知道，Google 一直在训练自动驾驶技术，他们遇到过各种各样奇怪的情况。有一次有几个小孩在高速公路上玩青蛙。还有一次，一个残疾人，坐着电动轮椅，在路中间追逐一只鸭子。鸭子绕圈跑，她也绕着圈追。那你说像这种情况你能一下子就准确预测这些人的行动路线吗？自动驾驶汽车识别路边的物体，都是靠把激光打到各种东西上再反射回来。可如果在下雪或者下雨，激光可能打到雪花或者雨滴上反射，汽车就可能对周围物体有重大误判。

计算机能不能保证看懂路边标记限速、慢行的交通标志牌？图形识别技术非常难，别忘了 Google 曾经把奥巴马夫人米歇尔给识别成一只猩猩。假如标志牌有损坏，或者上面被人贴了小广告，那汽车就很可能无法识别。还有，现在自动驾驶汽车都高度依赖 GPS 定位。可是现在美国有一种50美元就能买到的装置，能在周围干扰 GPS 信号。那如果路上有人使用这个装置，自动驾驶汽车要怎么办？

2016年，开特斯拉的一个司机车违反规定，把车完全交给自动驾驶，结果因为汽车没有识别出来前面的一辆白色卡车，导致死亡。它可能以为那是天上的白云或者别的什么东西。当然这是司机犯了错误，但这恰恰也说明自动驾驶技术非常容易遭遇意外。

除了安全，自动驾驶还有道德问题。比如说你正在以很快的速度开车，突然发现前边有一群小学生在马路上打闹。要避让这些小学生，你就会撞到路边的建筑物墙上；而如果撞墙，你的生命安全就面临危险。请问在这种情况下，你是选择撞墙还是选择撞向小学生呢？有道德的人，比如我，肯定是宁可自己面对生命危险，也不能撞小学生。

现在有很多公司正在研究自动驾驶的道德规范，Google 甚至还专门聘请了哲学家，但是没有研究出来什么令人满意的方案。奔驰公司已经宣布，他们对自动驾驶汽车的设定是优先保证自己车里司机和乘客的安全。在前面那种情况下，奔驰的车会果断撞向小学生。

那你说这不是杀手汽车吗？这种汽车怎么能上路呢？所以这就出现了一个道德困境。人在现场不管做出怎样的临时反应，我们都认为是正常的。可是人工智能不管事先怎么设定，我们都觉得别扭。

自动驾驶技术还有一个经济学问题。机器学习是高度依赖数据的。我们知道有个“二八法则”，说的是你花20%的时间就能解决80%的问题，剩下80%的时间解决20%的问题。对自动驾驶汽车来说，我看更可能是你用2%的数据就能训练一个能解决路面80%的情况的自动驾驶系统，但是剩下那20%的情况，你就是再用98%的数据也未必能解决。

美国50个州都有各自的交通法规，各地的气候条件和路况都不一样，这还不算美国和中国的差别。这意味着什么呢？这意味着在一个地区训练出来的自动驾驶技术，换一个地方就可能不好使。所以人工智能模型不能推广，你必须在每一个地区都采集大量的数据才行。

那好，谁拥有这么多数据呢？现在冒出来很多搞自动驾驶技术的公司，我非常怀疑他们怎么跟 Google 竞争。Google 一直都在积累数据，算法都是现成的，真正值钱的是数据。谁掌握了数据，谁的自动驾驶技术才有市场。人工智能时代的商业帝国一定是数据帝国，小的创业公司将会越来越难以起步。

所以，我们可以看到，即便是在“自动驾驶”这样看上去几乎纯工程领域的问题上，人工智能也有非常大的局限。第一，它不安全。第二，它不道德。第三，它不能促进商业平等，它只会让强大的公司变得更加强大。因此，在可以预见的未来，也许我们还是得自己开车。

这是我们说的第二部分内容，接下来我们说一说第三部分内容，也就是现阶段人工智能的社会化应用的根本矛盾是什么？

###  



在这一部分中，我们要讲一个小道理和一个大道理。

大道理很容易理解，但是这个小道理比较烧脑，相当于是一道数学题，而且是2016年才被人想明白，还专门发表了学术论文。不过，你并不需要任何高级的数学知识，我会帮你从直观上理解这个道理。

这个事儿得从美国的司法制度说起。中国的犯人，理论上来说，不管是什么人，同样的罪行就意味着同样的刑期。在监狱里如果表现好，同样的表现等于同样的减刑。惩罚力度，跟你是个什么人，没多大关系。但是在美国，法官判决的时候，会看犯人“是个什么人”。同样的罪行，如果这个人平时表现良好，就可能得到轻判。

这个指导思想是看犯人将来再次犯罪的可能性大小。如果法官认为这个人不会再次犯罪，他就会给轻判，而且很容易批准假释。而如果法官判断这个人将来很可能再次犯罪，那就会从重从严判罚，假释减刑什么的也别想了，就在监狱里待着，省得出去危害社会。

那怎么知道一个人是什么人呢？以前都是靠法官的主观判断，肯定是不太靠谱，所以现在是用算法来判断。有个私人公司叫 NorthPointe，开发了一个算法，叫 COMPAS，专门用来判断一个犯人再次犯罪的可能性大小。COMPAS 算法会考察一个犯人的100多项指标，然后给他打一个分。从1分到10分，分数越高，代表将来再犯罪的概率就越高。

这就比法官个人的判断强多了，毕竟哪个法官也不可能同时考虑这么多项指标。算法是冰冷的，但算法也是客观冷静的。

这个算法的原理都用大数据，根据以往的经验，你考察若干个关键指标，对一件事作出判断。训练算法用的都是美国的数据，犯人都是美国的犯人。美国的犯罪环境变化也不大，这一套完全合理。而且这个 NorthPointe 公司还非常痛快，把自己用的所有数据都在网上公布了，你可以随便研究。它具体的算法是保密的，但是如果你不服，你可以自己开发一套算法跟它比。研究者发现 COMPAS 算法的准确率还真可以，算法给打了1分的犯人，再次犯罪率只有22%；而打10分的犯人再次犯罪率高达81%。

一般这种犯罪率判断，最怕被人指责种族歧视。众所周知黑人的犯罪率比较高，那有没有可能，仅仅因为犯人是黑人，你就调高他的分数呢？COMPAS 算法在这一点上做得很绝，它根本就不考虑犯人是不是黑人。100多项指标包括性别、年龄、以前的犯罪历史等等，其中根本就没有种族这个项目。事实上人们统计发现，算法给打了7分的犯人中，如果是白人，后来的再次犯罪率是60%；而7分的黑人，再次犯罪率是61%，几乎相等。所以这个算法真的没有对黑人的种族歧视。

但还是有人对 NorthPointe 公司提出了种族歧视的指控。有个专门为了社会责任搞深入新闻调查的非盈利媒体，叫 ProPublic，研究了一下 COMPAS算法。我们知道算法预测的都是概率而已，哪怕是被打了5分以上的分数，犯人也不一定会再次犯罪。如果一个犯人明明没有后来再犯罪，却被算法打了个高分，那他就等于被算法冤枉了。

ProPublic 专门统计这种没有再犯罪，却被打了5分以上的情况。同样是后来没再犯罪的犯人，黑人被打高分的可能性是42%，而白人只有22%。那这不就是说，算法还是歧视了黑人吗？这个发现引发了轩然大波。由于具体的推导过程比较复杂，所以我们只说结论。导致这个差别的原因是，黑人总体再犯罪率高，黑人打高分的比例就必然高。每个分数代表的人群中被冤枉的比例是固定的，那既然黑人总体打高分的比例高，那么其中被冤枉的黑人就一定更多。

为了方便理解，我再换个说法。假设现在有个外星人，他根本不知道地球上的人还分为黑人和白人，他以为所有人都是一样的。那么他拿到数据之后，会认为这是一套公平的打分系统。可是当我们把黑人白人的标签放进去之后，却发现黑人被冤枉的比例更高。而个别黑人之所以吃亏，是因为黑人整体的再犯罪率高。

那怎么避免这个现象呢？从数学角度根本避免不了。如果你想保护黑人，给黑人少打一些高分，那你的分数系统就是不准确的。同样一个分数就不能代表同样的再犯罪率，法官就没法从这个分数系统中获得正确的参考意见。

所以到底什么叫公平？程序员追求的是分数系统的准确性，记者要求的是不能冤枉黑人，而从数学上来说，这两个要求不可能同时满足，这是一个根本的矛盾。只要用过去的经验去预测未来，就一定有这个矛盾。产生经验的是一批人，要被预测影响的却是另外一批人。这就相当于新人要为前人犯的过错承担后果。

其实这个矛盾在生活中普遍存在。比如在上海这样，基本不存在重男轻女的大城市里，女生的考试成绩普遍比男生好。那如果根据这个规律，为了提高学校的总体成绩，在高一入学的时候尽量多录取女生，这不就是对男生的歧视吗？你的经验很可能是准确的，但是你是在用以前人的表现去惩罚后来的人。你是在让一个人为不是他自己的行为付出代价。

这是一切基于经验的决策的本质缺陷。人工智能再厉害，只要是基于经验的，只要预测不是100%准确，就一定会有人被冤枉。所谓“公平”，其实是你的主观选择。选择算法准确度的公平，你就会冤枉一些特定的黑人；选择不冤枉黑人，你的算法就不准确，你就会冤枉别的人。算法可以根本不考虑种族，但种族就隐藏在数据之中。丑小鸭定理说，一切分类都是主观的。有分类就会有歧视，此事古难全。人工智能能给我们的决策提供很大的方便，但社会还是这个社会，数学还是同样的数学，人工智能改变不了问题的本质。

###  



好，这本书的主要内容，我们已经介绍的差不多了，下面我们来总结一下。

我们谈起技术进步，通常听到的都是好消息。比如偏边远地区不是送货难吗？我们可以用无人机送货。农村教育资源有限是吧？我们可以让农民看电视学技术啊。

技术肯定能给一部分人带来方便，但是要想全面解决一个社会问题，那可就太难了。就算是看似成功的实践，背后也不知道有多少意想不到的情况。

我在大学曾经选修过一门课叫《软件工程》。当时软件工程在中国刚刚起步，还没有多少开软件公司一夜暴富的神话，但是给我们讲课的老师当时在外面也接了一些小活儿，他有丰富的实践经验。

比如，他帮一个超市做进货、销售和存货的计算机管理系统，系统做好了，运行起来之后发现计算机显示的库存和实际总对不上账。老板就说你这个系统是不是没搞好啊？其实不是系统有问题，而是超市员工一直都在偷拿东西。在这样的情况下，你怎么还能把软件给做成和运行起来，这也是软件工程的一部分。

所以，技术的确经常能让事情更有效率，但写程序的是人，录入数据的是人，使用系统的是人。如果不解决人的问题，技术终究不能解决真正的社会问题。

撰稿：万维钢
脑图：摩西脑图工作室
转述：怀沙

### 划重点

 1.计算机和人工智能擅长解决定义良好的工程问题，而社会问题往往不是定义良好的。

 2.在自动驾驶这个看似纯工程的领域，人工智能也存在很大的局限：不安全，不道德，不能促进商业平等。

 3.对于公平，人类社会存在不同的标准，人工智能无法改变社会问题的本质。

